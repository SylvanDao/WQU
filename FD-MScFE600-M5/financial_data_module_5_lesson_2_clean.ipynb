{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2V43v2MT9M6V"
   },
   "source": [
    "MODULE 5 | LESSON 2\n",
    "\n",
    "# **News Data**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U255eqIW7HPa"
   },
   "source": [
    "|  |  |\n",
    "|:---|:---|\n",
    "| **Reading Time**  |  45 minutes  |\n",
    "| **Prior Knowledge**  |  Familiarity with Python programming: Basic Python syntax, data manipulation, DataFrames, yfinance, sklearn; <br>Fundamental financial concepts: stocks, market sentiment, risk management, and trading strategies, sentiment analysis  |\n",
    "| **Keywords**  | Non-negative Matrix Factorization (NMF), Feature Extraction, Parts-based Representation, Sentiment Analysis, <br>Natural Language Processing (NLP), Sentiment Analysis, Topic Modeling, FinBERT, NMF, TF-IDF |\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rDtX3qRG48R4"
   },
   "source": [
    "*In this lesson, we demonstrate how to use news data and natural language processing (NLP) techniques for financial analysis, specifically for Microsoft. We will explore different news sources, perform sentiment analysis using FinBERT, and use topic modeling (NMF) to uncover hidden themes in the news. The goal is to gain insights that could potentially lead to better-informed financial decisions.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading libraries\n",
    "\n",
    "import datetime\n",
    "import feedparser\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests\n",
    "import yfinance as yf\n",
    "\n",
    "from datetime import datetime, timedelta\n",
    "from scipy.special import softmax\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import NMF\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "# Prompt the user for the API key\n",
    "api_key = input(\"Enter your News API key: \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wKsDN79P_Ear"
   },
   "source": [
    "# **1. Purpose of News Data**\n",
    "\n",
    "Exploring news data is crucial for informed financial and investment decision-making. Timely and accurate news analysis empowers investors to navigate complex markets effectively. In the dynamic world of finance, where markets fluctuate based on a multitude of factors, news data plays a pivotal role. Here are several reasons why exploring news data is crucial for informed financial and investment decisions:\n",
    "\n",
    "**Market Sentiment Analysis:**\n",
    "\n",
    " - **Emotional Drivers:** News articles capture investor emotions, expectations, and reactions. Positive news can boost confidence, leading to increased investments, while negative news may trigger caution or even sell-offs.\n",
    " - **Behavioral Insights:** Understanding market sentiment helps investors anticipate trends and align their strategies accordingly.\n",
    "\n",
    "**Event Impact Assessment:**\n",
    " - **News-Driven Volatility:** Events such as earnings reports, geopolitical developments, or regulatory changes significantly impact stock prices. Analyzing news allows us to assess the potential effects of these events on specific assets or entire sectors.\n",
    " - **Timeliness Matters:** Investors who react swiftly to breaking news gain a competitive edge.\n",
    "\n",
    "**Risk Management:**\n",
    " - **Early Warnings:** News provides early warnings about risks. Staying informed helps to mitigate risks, whether it’s a sudden economic downturn, a corporate scandal, or a regulatory shift.\n",
    " - **Portfolio Adjustments:** By monitoring news, investors can adjust their portfolios proactively, minimizing exposure to adverse events.\n",
    "\n",
    "**Sector Insights:**\n",
    " - **Industry Trends:** News highlights emerging trends, mergers, acquisitions, and disruptions within specific sectors. Investors can allocate funds based on sector-specific news.\n",
    " - **Sector Rotation:** Understanding which sectors are gaining momentum or facing challenges informs strategic asset allocation.\n",
    "\n",
    "**Algorithmic Trading:**\n",
    " - **News-Driven Algorithms:** Algorithmic trading models rely on real-time news feeds. These algorithms react swiftly to breaking news, executing trades based on predefined rules.\n",
    " - **Quantitative Strategies:** Incorporating news sentiment scores into quantitative models enhances their predictive power.\n",
    "\n",
    "**Behavioral Finance:**\n",
    " - **Herding Behavior:** News influences investor behavior. Herding (following the crowd) or panic selling during market downturns can be better understood through news analysis.\n",
    " - **Fear of Missing Out (FOMO):** News-driven biases impact decision-making. Recognizing these biases helps investors make more rational choices.\n",
    "\n",
    "In summary, news data isn’t just information; it’s a compass guiding financial decisions. By analyzing news comprehensively, investors gain insights, manage risks, and adapt to the ever-evolving landscape of global markets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lzgqZCUTYK2v"
   },
   "source": [
    "#**2. Sources of News Data**\n",
    "\n",
    "There's no single news data API that's universally used by all financial engineers. The choice of API often depends on specific needs, budget, and the type of analysis being performed. However, here are some popular and reputable news data APIs commonly used in the financial industry:\n",
    "\n",
    "1. **Refinitiv (formerly Thomson Reuters)**:\n",
    " - Widely used by institutional investors and professional traders.\n",
    " - Offers comprehensive news coverage: Global markets, equities, fixed income, commodities, FX, etc.\n",
    " - Advanced features: Sentiment analysis, entity recognition, real-time alerts.\n",
    " - Integration with Refinitiv Eikon platform: Provides access to a vast range of financial data and analytics tools.\n",
    "\n",
    "2. **Bloomberg**:\n",
    " - Another industry-standard for financial professionals.\n",
    " - Extensive news coverage: Global markets, business news, economic data, company information, worldwide televised broadcasts\n",
    " - Powerful analytics and data visualization tools.\n",
    "Integrated with Bloomberg Terminal: Provides a comprehensive suite of financial data and tools.\n",
    "\n",
    "3. **FactSet**:\n",
    " - Popular among investment professionals and asset managers.\n",
    " - Strong focus on fundamental data and company research.\n",
    " - Offers news coverage: Market news, company announcements, economic data.\n",
    " - Integrated with FactSet workstation: Provides access to a wide range of financial data and analytics.\n",
    "\n",
    "4. **Dow Jones (Factiva)**:\n",
    " - Well-known for its business and financial news coverage.\n",
    " - Extensive historical archive: Access to news articles dating back decades.\n",
    " - Global coverage: News from various regions and languages.\n",
    " - Often used for sentiment analysis and text mining.\n",
    "\n",
    "When choosing an API, the most important considerations are:\n",
    " - **Cost:** These APIs typically involve subscription fees, which can vary significantly based on data access, features, and usage volume.\n",
    " - **Data Coverage:** Each API has its strengths in terms of the types of news and markets it covers. Choose one that aligns with your specific needs.\n",
    " - **Integration:** Consider how easily the API integrates with your existing tools and workflows.\n",
    "\n",
    "Unfortunately, the APIs used by financial engineers listed above (Refinitiv, Bloomberg, FactSet, Dow Jones) are typically not free to use. They are commercial products targeted toward professional investors and institutions, and they usually involve subscription fees that can be quite substantial. However, there are a few other alternative options available for free or limited free access to get news data, some of which we will consider in detail in this lesson.\n",
    "\n",
    "**Important Note:** When using free options, be mindful of usage limits and terms of service to avoid exceeding quotas or violating any restrictions.\n",
    "\n",
    "If you have specific budget constraints or are just starting to explore news data analysis, the free options can be a good starting point. However, for professional-grade financial analysis, investing in a commercial API will likely be necessary to access the depth and breadth of data required.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7kieZ1LpOV1b"
   },
   "source": [
    "## **2.1 Using `yfinance` for News Data**\n",
    "\n",
    "While `yfinance` is primarily known for historical stock price data, it also provides access to financial news related to specific stocks.\n",
    "\n",
    "For example, we can use the `yfinance` library to get stock information for Microsoft. We can leverage the fact that it's typically a list of dictionaries. Each dictionary represents a news article and contains keys like `title`, `publisher`, `link`, `published`, etc.\n",
    "\n",
    "Once we have all the interesting details, we can print the news data, and we can also create a DataFrame from specific pieces of information to keep our data for further manipulations and exploratory data analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Ticker object for Microsoft and access the news data\n",
    "msft = yf.Ticker(\"MSFT\")\n",
    "news_data = msft.news\n",
    "\n",
    "# Print the desired information for each article\n",
    "for article in news_data:\n",
    "    content = article.get('content')\n",
    "    if content:\n",
    "        # Assuming title, publisher, link are now within 'content'\n",
    "        print(\"Published Time:\", content.get('pubDate'))\n",
    "        print(\"Title:\", content.get('title'))  \n",
    "        print(\"Publisher:\", content.get('provider').get('displayName'))  \n",
    "        print(\"Link:\", content.get('canonicalUrl').get('url'))\n",
    "        print(\"Content Type:\", content.get('contentType'))\n",
    "        print(\"-\" * 30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JARSvrMqR43C"
   },
   "source": [
    "As you notice, here we only have 8 articles. `yfinance` does have some limitations when it comes to retrieving news articles.\n",
    "\n",
    " - **Limited Number of Articles:** `yfinance` doesn't provide a direct way to control the number of news articles it fetches. The number of articles returned can vary and seems to be capped, often resulting in a smaller dataset than you might expect. The exact limit is not explicitly documented and might depend on factors like the stock ticker, news availability, and the underlying data source used by `yfinance`.\n",
    "\n",
    " - **Reliance on External APIs:** `yfinance` doesn't have its own dedicated news database. It relies on aggregating news from various external sources and APIs. This means the availability and quantity of news data can be influenced by the limitations and potential changes in those external sources.\n",
    "\n",
    " - **No Fine-grained Control:** `yfinance` doesn't offer options to filter news articles by specific criteria like date range, news provider, or keywords. You get a general set of recent news articles related to the stock ticker, but you can't customize the query further.\n",
    "\n",
    "\n",
    "In summary: While `yfinance` is a convenient tool for getting basic stock information and a quick overview of recent news, it's not ideal for comprehensive news analysis or if you need a large and customizable news dataset. But despite limitations, `yfinance` still has variable use cases, especially when you need quick and easy access to financial data.\n",
    "\n",
    " - **Simple News Monitoring:** Getting a snapshot of recent news related to a stock. While limited, the news feature in `yfinance` can be helpful for staying updated on major developments or headlines affecting a company.\n",
    "\n",
    " - **Prototyping or educational purposes:** It's a great tool for learning about financial data analysis or for quickly testing ideas without the overhead of more complex data sources.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nFT116CtIXPc"
   },
   "source": [
    "## **2.2 RSS Feeds: Google News RSS**\n",
    "\n",
    "An alternative way of getting news updates is to subscribe to an RSS feed. RSS stands for Really Simple Syndication or Rich Site Summary. It's a standardized web feed format that allows users to subscribe to updates from websites or blogs. These updates are typically delivered as news headlines, article summaries, or other content changes. Many well-known financial news sources provide RSS feeds. Some examples include the *Wall Street Journal*, Bloomberg, Reuters, *Financial Times*, Seeking Alpha, The Motley Fool, Benzinga, etc.\n",
    "\n",
    "In this lesson, we explore using Google News RSS to access news articles from Google News in a structured format that can be easily processed by machines. While not strictly an API, we can parse Google News RSS feeds for free using libraries like `feedparser` Python library designed to parse syndicated feeds, most commonly RSS and Atom feeds. It can handle various feed formats and variations, making it a versatile tool for extracting information from different sources.\n",
    "\n",
    "In the following code, we define the RSS feed URL and extract article information (title, link, publication date, etc.) for the query term \"Microsoft\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define RSS feed URL and retrieve feed\n",
    "query = \"Microsoft\"\n",
    "rss_url = f\"https://news.google.com/rss/search?q={query}t&hl=en-US&gl=US&ceid=US:en\"\n",
    "feed = feedparser.parse(rss_url)\n",
    "\n",
    "for entry in feed.entries:\n",
    "    print(\"Published:\", entry.published)\n",
    "    print(\"Title:\", entry.title)\n",
    "    print(\"Link:\", entry.link)\n",
    "    print(\"-\" * 30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ho2KPog4wbE0"
   },
   "source": [
    "As you can see here, we have more news titles than we get from `yfinance`. But again, Google News RSS also has its advantages and limitations:\n",
    "\n",
    "**Advantages of Google News RSS:**\n",
    " - Free Access: You can access and parse Google News RSS feeds without any API keys or subscription fees.\n",
    " - Wide Range of Topics: Google News covers a vast array of news categories and topics.\n",
    " - Fresh Content: RSS feeds are updated frequently, so you get access to the latest news articles.\n",
    "\n",
    "**Limitations:**\n",
    " - No Fine-grained Control: You can't filter articles by specific criteria like date range or news source within the RSS feed itself.\n",
    " - Potential Rate Limiting: Google might impose rate limits on how frequently you can access their RSS feeds.\n",
    " - No Historical Data: RSS feeds typically provide only recent articles, not a historical archive.\n",
    "\n",
    "**Use Cases:**\n",
    " - Staying Updated on Specific Topics: Subscribe to RSS feeds for topics relevant to your interests or investments.\n",
    " - Building Simple News Aggregators: Create a basic news aggregator that displays articles from various Google News RSS feeds.\n",
    " - Sentiment Analysis and Text Mining: Extract text from news articles for sentiment analysis or other text-based research.\n",
    "\n",
    "Overall, Google News RSS is a valuable resource for accessing free news data, especially for smaller projects, personal use, or when you need a quick overview of recent news on specific topics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GojqTimI16Ui"
   },
   "source": [
    "## **2.3 News API**\n",
    "\n",
    "Although not a Python package, News API provides a straightforward way to fetch news data. It offers a free tier with limited requests per day. This might be sufficient for small-scale projects or learning purposes. You will need to sign up for a free API key on the NewsAPI website before using it.\n",
    "\n",
    "News API is a cloud-based REST API that provides programmatic access to a vast collection of news articles from thousands of sources around the world.\n",
    "It aggregates news from reputable publishers, news agencies, blogs, and other online media outlets.\n",
    "\n",
    "**Compared to `yfinance` and Google News RSS:**\n",
    " - More Articles: You can potentially retrieve a much larger number of news articles with News API.\n",
    " - Customization: You have much finer control over the news data you fetch through search and filtering options.\n",
    " - Additional Features: Sentiment analysis and various API endpoints add more analytical capabilities.\n",
    "\n",
    "**Limitations:**\n",
    " - Cost: While it offers a free tier with limited requests, you'll need a paid subscription for larger-scale usage or access to all features.\n",
    " - Rate Limits: Even with paid plans, there are rate limits on how many requests you can make within a given time period.\n",
    "\n",
    "**Use Cases:**\n",
    " - News Monitoring and Analysis: Track news trends, identify emerging topics, and analyze media coverage for specific companies, industries, or events.\n",
    " - Sentiment Analysis: Gauge public sentiment toward brands, products, or political figures.\n",
    " - Market Research: Gather insights on consumer behavior, competitor activity, and industry trends.\n",
    " - Content Curation: Build news aggregators or personalized news feeds.\n",
    " - Algorithmic Trading: Incorporate news sentiment and events into trading strategies.\n",
    "\n",
    "Overall, News API is a more powerful and versatile tool for news data analysis compared to `yfinance` and Google News RSS. It's a great option if you need a larger dataset, more customization, and additional features like sentiment analysis. However, it does come with a cost for more extensive usage.\n",
    "\n",
    "For the remainder of this lesson, we will explore some of the News API features. First, we need to sign up and get an API key.\n",
    "\n",
    " - Sign Up: Create a free account on the News API website: https://newsapi.org/pricing, selecting the free Developer pricing plan. Sign up as an individual. The free tier offers limited requests per day and allows you to search news articles (with a 24-hour delay) up to a month old, but this should suffice for learning purposes.\n",
    " - Get Your API Key: As soon as you sign up, you'll be greeted with your unique API key that you'll need to include in your requests.\n",
    " - On the greeting page, you will also find a link to the \"Getting Started Guide.\" Please take some time to explore the API documentation for detailed information on endpoints, parameters, and response formats.\n",
    "\n",
    "In this lesson, we will explore some basic functionality and experiment with different queries and filters to tailor the results to our specific needs.\n",
    "\n",
    "The following is the example code that fetches news articles related to \"Microsoft\" from News API, saves them in DataFrame, and handles potential errors during the process. Do not forget to get your own API key. Then, please navigate to the top of this lesson and replace `API_KEY` with your actual API key in the code cell at the top of this notebook. Please rerun that code cell and then proceed with the following code cell below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define News API variables\n",
    "query = \"Microsoft\"\n",
    "url = f\"https://newsapi.org/v2/everything?q={query}&apiKey={api_key}&language=en\"\n",
    "response = requests.get(url)\n",
    "\n",
    "# Get news data\n",
    "results = []\n",
    "if response.status_code == 200:\n",
    "    news_data = response.json()\n",
    "    for article in news_data['articles']:\n",
    "        results.append({\n",
    "            'Date': article['publishedAt'][:10],  # Extract date\n",
    "            'URL': article['url'],\n",
    "            'Source': article['source']['name'],\n",
    "            'Author': article['author'],\n",
    "            'Title': article['title'],\n",
    "            'Description': article['description'],\n",
    "            'Content': article['content']\n",
    "        })\n",
    "else:\n",
    "    print(\"Error fetching news:\", response.status_code)\n",
    "\n",
    "# Create DataFrame\n",
    "df = pd.DataFrame(results)\n",
    "df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XgXN3ZeEsaJ2"
   },
   "source": [
    "We can also save the dataset locally in a news_data.csv file. Saving News API data to a CSV file is beneficial because of the limitations on data availability for certain time periods, particularly with the free plan. It is highly probable that the remainder code in this lesson will output different results when you run it. Thus, referring to news_data.csv might be beneficial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save News dataset\n",
    "df.to_csv('news_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lpgUqP0Xy4H5"
   },
   "source": [
    "`if response.status_code == 200:` This line checks if the value of `response.status_code` is equal to 200. In the context of HTTP requests, a status code of 200 typically indicates a successful request.\n",
    "\n",
    "If the condition in the if statement is true (i.e., the status code is 200), this line executes and the code execution will move to next line. `response.json()` in the following line is a method that attempts to parse the response content as JSON (JavaScript Object Notation). JSON is a common data format used for data exchange on the web. The parsed JSON data is then assigned to the variable `news_data`.\n",
    "\n",
    "The code output saves article metadata for each news item: information like publication date (publishedAt), source (source), author (author), title (title), description (description), content (content), and URL (url) provide context for the sentiment analysis.\n",
    "\n",
    "Note the \"[Removed]\" in the Content column of dataframe. This is likely due to content restrictions imposed by the news source or by News API itself. The reasoning behind this is as follows:\n",
    "\n",
    " - Publisher Restrictions: Some news publishers may choose to limit the distribution of their full article content through APIs. They may only provide headlines, summaries, or partial content. In such cases, News API might replace the unavailable content with \"[Removed]\".\n",
    " - News API Policies: News API itself might have policies in place to prevent the scraping or redistribution of full articles. They may intentionally remove or truncate content to comply with copyright regulations or publisher agreements.\n",
    " - Content Filtering: In some cases, News API might apply content filtering to remove sensitive or inappropriate content. This could also result in \"[Removed]\" appearing in place of the filtered content.\n",
    "\n",
    "Unfortunately, there's no direct way to retrieve the removed content through News API. We would need to access the full article through the original source's website if it is available."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P3nsYoib2nQx"
   },
   "source": [
    "The News API allows you to **filter news articles by category**. We can specify the category parameter in an API request to retrieve news from a specific category, such as business, entertainment, general, health, science, sports, and technology.\n",
    "\n",
    "Important considerations:\n",
    " - Category Availability: The availability of certain categories might vary depending on News API plan and the geographic region we are targeting.\n",
    " - Relevance: While the business category is a good starting point for finance news, it might also include articles that are not strictly finance-related. We might need to further filter the results based on keywords or other criteria to refine the selection.\n",
    " - Language and Country: We can further refine search by specifying the language and country parameters to retrieve news from a specific region and in a particular language.\n",
    "\n",
    "Unfortunately, the News API does not directly support searching for multiple categories simultaneously using the category parameter. We can only specify one category at a time. However, we can achieve a similar result by making separate API requests for each category and then combining the results.\n",
    "\n",
    "A significant limitation of searching by category is that NEWS API currently does not support category parameter on `https://newsapi.org/v2/everything` endpoint. Instead, we should use `https://newsapi.org/v2/top-headlines` endpoint. This endpoint allows filtering by category. However, this endpoint has some limitations compared to `/everything`:\n",
    " - Limited Articles: It returns a limited number of articles (usually the top 20-30 headlines) for a given query and category. It's not meant for comprehensive news retrieval.\n",
    " - Focus on Recent News: It primarily focuses on recent news and might not include older articles.\n",
    " - Source Restrictions: It sources articles only from a limited set of popular and well-known news sources, potentially missing out on smaller publications.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bgGIU8mDZOTO"
   },
   "source": [
    "# **3. Sentiment Analysis**\n",
    "\n",
    "Financial traders can leverage news data and sentiment analysis in several ways to enhance their decision-making processes. News data and sentiment analysis empower traders to make informed decisions, manage risks, and adapt to market dynamics. By combining quantitative models with qualitative insights, traders gain a competitive edge.\n",
    "\n",
    "**How Financial Traders Might Use Sentiment Analysis:**\n",
    " - Identify Market Sentiment: Analyze the overall sentiment of news articles related to a specific stock, sector, or the entire market to gauge investor sentiment and predict potential price movements.\n",
    " - Event-Driven Trading: Detect news events (like earnings announcements, product launches, regulatory changes) and analyze their sentiment to identify trading opportunities.\n",
    " - News-Based Sentiment Indicators: Create custom indicators based on news sentiment to supplement technical analysis and inform trading decisions.\n",
    " - Risk Management: Monitor news sentiment to assess potential risks and adjust portfolio positions accordingly.\n",
    "\n",
    "**Important Considerations:**\n",
    " - Accuracy of Sentiment Analysis: Sentiment analysis is not always perfect, especially for financial news, which can contain complex language and jargon. It's crucial to validate the accuracy of your sentiment analysis models.\n",
    " - Timeliness: News sentiment can change rapidly. For trading purposes, real-time or near-real-time news feeds and sentiment analysis are often required.\n",
    " - Integration with Trading Systems: You'll need to integrate the news sentiment data with your trading platform or algorithms to automate trading decisions based on the analysis.\n",
    "\n",
    "Remember that sentiment analysis is just one tool in a financial trader's toolkit. It should be used in conjunction with other forms of analysis and risk management strategies.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T3PEcXQU4N9d"
   },
   "source": [
    "## **3.1 Sentiment analysis using FinBERT**\n",
    "\n",
    "Unfortunately, for us, News API's Sentiment Analysis is a paid feature. If you have access to such a plan, the API response would already include sentiment scores (positive, negative, neutral) for each article. You can then use these scores directly in your trading strategies.\n",
    "\n",
    "In this lesson, we will use an alternative method by implementing the sensitive scoring function manually. For this, we will use **FinBERT**, a pre-trained NLP model specifically designed for financial text. It's based on the BERT architecture (Bidirectional Encoder Representations from Transformers), a type of Transformer model, and has been fine-tuned on a large corpus of financial data.\n",
    "\n",
    "**Transformers** are a type of deep learning model that have been very successful in natural language processing (NLP) tasks. They are particularly good at understanding the relationships between different words in a sentence, which is important for tasks like sentiment analysis. FinBERT is a specific type of Transformer model that has been pre-trained on a large corpus of financial text. This means that it has already learned a lot about the language used in financial documents. This makes it particularly good at understanding the nuances of financial language, including jargon, sentiment, and specific financial concepts, and this makes it well suited for tasks like sentiment analysis of financial news articles.\n",
    "\n",
    "In the following piece of code, we utilize the FinBERT model to apply a sentiment score to each news article. To do this, we first need to preprocess the content of each article using the `preprocess` function. Then, we apply `get_sentiment()` to perform sentiment analysis on a given preprocessed text using the FinBERT model. Here's how it works:\n",
    "\n",
    " - **Tokenization:** The preprocessed text is tokenized using the tokenizer object. Tokenization is the process of breaking down the text into individual units (tokens) - words or subwords - that the model can understand. The `return_tensors='pt' ` argument specifies that the tokens should be returned as PyTorch tensors. PyTorch tensors are multi-dimensional arrays that are a fundamental data structure in the PyTorch library. They are similar to NumPy arrays but with some key advantages. As an analogy, think of PyTorch tensors as containers that hold numerical data that can be of various dimensions (e.g., 1D for vectors, 2D for matrices, 3D or higher for more complex data).\n",
    " - **Model Inference and Score Extraction using Softmax:** The tokenized input is passed to the FinBERT model for inference. The relevant scores are extracted from the model's output and converted to a NumPy array using `detach().numpy()`. The scores are then passed through the `softmax` function. Softmax converts the scores into probabilities, ensuring that they sum up to 1.\n",
    "\n",
    "The `get_sentiment()` function returns a dictionary containing the probabilities for negative, neutral, and positive sentiment, with the highest probability first.\n",
    "\n",
    "The following code snippet processes the response from the News API and performs sentiment analysis (by successive implementation of text preprocessing, tokenization, model inference, and getting a sentiment) on the 'Description' of each retrieved article. We are also dropping rows where 'Description' has been removed and the article is no longer available:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the FinBERT model\n",
    "MODEL = f\"ProsusAI/finbert\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(MODEL)\n",
    "\n",
    "# Text processing function\n",
    "def preprocess(text):\n",
    "    if text is None: # Handle None values by returning an empty string if text is None\n",
    "        return \"\"\n",
    "    new_text = []\n",
    "    for t in text.split(\" \"):\n",
    "        t = '' if t.startswith('#') and len(t) > 1 else t  # remove hashtags\n",
    "        t = '' if t.startswith('@') and len(t) > 1 else t  # remove usernames\n",
    "        t = '' if t.startswith('http') else t  # remove URLs\n",
    "        new_text.append(t)\n",
    "    return \" \".join(new_text)\n",
    "\n",
    "# Sentiment scoring function\n",
    "def get_sentiment(text):\n",
    "    text = preprocess(text)\n",
    "    encoded_input = tokenizer(text, return_tensors='pt')\n",
    "    output = model(**encoded_input)\n",
    "    scores = output[0][0].detach().numpy()\n",
    "    scores = softmax(scores)\n",
    "    return {\n",
    "        'positive': scores[0],\n",
    "        'negative': scores[1],\n",
    "        'neutral': scores[2]\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ni84MjfTLZ-2"
   },
   "source": [
    "Now that we have initiated the FinBERT model and constructed helper functions, we can apply these techniques onto our `df` News data DataFrame. Please note that by the time you run the code in this lesson, availability of news data via the News API free plan would have changed since the plan allows one month of history. We saved the original News data when we wrote this lesson. You will need to load the 'WQU_FD_news_data.csv' file in order to get the same results in this lesson. Otherwise, please proceed to explore fresh data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open saved DataFrame - OPTIONAL\n",
    "df = pd.read_csv('WQU_FD_news_data.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove rows with \"[Removed]\" or None in Description\n",
    "df = df[df['Description'] != '[Removed]']\n",
    "df = df.dropna(subset=['Description'])\n",
    "\n",
    "# Apply sentiment scoring to the 'description' column and create new columns\n",
    "df.loc[:, 'Sent_positive'] = df['Description'].apply(lambda x: get_sentiment(x)['positive'])\n",
    "df.loc[:, 'Sent_negative'] = df['Description'].apply(lambda x: get_sentiment(x)['negative'])\n",
    "df.loc[:, 'Sent_neutral'] = df['Description'].apply(lambda x: get_sentiment(x)['neutral'])\n",
    "df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SHK9KSutzLYR"
   },
   "source": [
    "The code output contains sentiment scores. This is how to interpret sentiment:\n",
    "\n",
    " - Higher probability indicates stronger sentiment. For example, if `positive: 0.85`, it suggests a strong positive sentiment expressed in the article.\n",
    " - Look for dominant sentiment. The category with the highest probability usually represents the overall sentiment of the article.\n",
    " - Consider the context. Even with high sentiment scores, it's important to read the article content to understand the nuances and specific aspects driving the sentiment.\n",
    "\n",
    "By analyzing the sentiment scores alongside the article content, we can gain insights into the overall sentiment toward Microsoft as reported in the news. This information can be valuable for understanding market perception, identifying potential trends, and making informed decisions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Oxo8Wu9uhWWm"
   },
   "source": [
    "## **3.2 Visualizing Sentiment Scores**\n",
    "\n",
    "We can now visualize sentiment scores. But before doing this, note that there are multiple articles on some days. Directly summing up the sentiment scores when there are multiple articles on the same day can lead to inflated values and spikes in the plot. To address this, we should normalize the sentiment scores by the number of articles on each day before plotting. By normalizing the sentiment scores, the plot should now show a more accurate representation of the overall sentiment trend without being affected by the number of articles on each day. The spikes should be reduced, and the plot will be more interpretable.\n",
    "\n",
    "In the following code snippet we group articles by date and aggregate to get both the sum and count of articles for each date. Then, we divide the sum of each sentiment score by the count of articles for that date to get the average sentiment for the day. This normalizes the scores and prevents inflation due to varying numbers of articles. Then, we use the normalized sentiment scores for visualization.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by date and calculate average sentiment scores\n",
    "df_grouped = df.groupby('Date').agg(['sum', 'count'])  # Get sum and count\n",
    "\n",
    "# Create new columns for normalized sentiment scores\n",
    "df_grouped['avg_positive'] = df_grouped['Sent_positive']['sum'] / df_grouped['Sent_positive']['count']  # Normalize positive\n",
    "df_grouped['avg_negative'] = df_grouped['Sent_negative']['sum'] / df_grouped['Sent_negative']['count']  # Normalize negative\n",
    "df_grouped['avg_neutral'] = df_grouped['Sent_neutral']['sum'] / df_grouped['Sent_neutral']['count']  # Normalize neutral\n",
    "\n",
    "# Convert df_grouped.index to DatetimeIndex\n",
    "df_grouped.index = pd.to_datetime(df_grouped.index)\n",
    "\n",
    "# Plot the sentiment data\n",
    "fig, ax = plt.subplots(figsize=(16, 6))\n",
    "width = 0.2\n",
    "ax.bar(df_grouped.index - pd.DateOffset(days=width), df_grouped['avg_positive'], width=width, label='Positive', color='green')\n",
    "ax.bar(df_grouped.index, df_grouped['avg_neutral'], width=width, label='Neutral', color='orange')\n",
    "ax.bar(df_grouped.index + pd.DateOffset(days=width), df_grouped['avg_negative'], width=width, label='Negative', color='red')\n",
    "\n",
    "# Set plot attributes (labels, ticks, title, legend, gridlines)\n",
    "ax.set_xlabel('Date')\n",
    "ax.set_ylabel('Average Sentiment')\n",
    "ax.set_title('Sentiment Analysis of News Articles')\n",
    "ax.legend()\n",
    "ax.grid(True, axis='y', linestyle='--')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3t1NX_MwSuDf"
   },
   "source": [
    "The resulting plot is a bar chart visualizing the average sentiment of news articles over time. There are three bars for each date, representing the three sentiment categories: positive (green), negative (red), and neutral (orange). The height of each bar indicates the magnitude of the average sentiment score for that category on the corresponding date. Taller bars indicate stronger sentiment.\n",
    "\n",
    "The plot allows us to observe trends in sentiment over time. We can see how the average sentiment changes across dates, whether it's becoming more positive, negative, or staying neutral. By comparing the heights of the bars for each category, we can identify periods of stronger positive or negative sentiment. For example, if on a particular date, the red bar (negative) is much taller than the green bar (positive), it suggests that the news articles for that date were generally more negative in sentiment. Conversely, a taller green bar would indicate more positive sentiment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NY7v8gUm-nye"
   },
   "source": [
    "## **3.3 Sentiment Analysis Scores in Conjunction with Stock Price**\n",
    "\n",
    "Let's consider the following example to see how we can use sentiment analysis scores in conjunction with Microsoft's stock price, keeping in mind the caveats about sentiment analysis accuracy and the need for other analysis tools.\n",
    "\n",
    "In this example, we will visualize sentiment and price trends. We'll plot sentiment scores alongside historical stock prices. This can help to visually identify potential correlations between sentiment shifts and price movements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch Microsoft stock data for the same date range\n",
    "msft = yf.download(\"MSFT\", start=df_grouped.index.min(), end=df_grouped.index.max())\n",
    "\n",
    "# Scale the Microsoft price data\n",
    "scaler = MinMaxScaler()\n",
    "scaled_stock_price = scaler.fit_transform(msft[['Close']])\n",
    "\n",
    "# Plot scaled price plot\n",
    "fig, ax = plt.subplots(figsize=(16, 6))\n",
    "ax.plot(msft.index, scaled_stock_price, color='blue', label='MSFT Adj Close Price (Scaled)')\n",
    "\n",
    "# Plot the sentiment data\n",
    "width = 0.2  # Adjust the width as needed\n",
    "ax.bar(df_grouped.index - pd.DateOffset(days=width), df_grouped['avg_negative'], width=width, label='Negative', color='red')\n",
    "ax.bar(df_grouped.index, df_grouped['avg_neutral'], width=width, label='Neutral', color='orange')\n",
    "ax.bar(df_grouped.index + pd.DateOffset(days=width), df_grouped['avg_positive'], width=width, label='Positive', color='green')\n",
    "\n",
    "# Set plot attributes (labels, ticks, title, legend, gridlines)\n",
    "ax.set_xlabel('Date')\n",
    "ax.set_title('Sentiment Analysis and Stock Price of Microsoft (scaled)')\n",
    "ax.set_ylabel('Value (Sentiment and Stock Price)')\n",
    "ax.legend()\n",
    "ax.grid(True, axis='y', linestyle='--')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Qjlv62Q013rE"
   },
   "source": [
    "Now the plot also shows Microsoft's stock price along with sentiment score bars. Please note that the stock price has been scaled, and we did this using MinMaxScaler. The MinMaxScaler from `sklearn.preprocessing` scales data to a specific range, typically between 0 and 1. It does this by applying the following formula:\n",
    "\n",
    "$$X_{\\text{scaled}} = \\frac{(X - X_{min})}{(X_{max} - X_{min})}$$\n",
    "\n",
    "Where:\n",
    "\n",
    " - $X$ is the original data;\n",
    " - $X_{\\text{scaled}}$ is the scaled data;\n",
    " - $X_{min}$ is the minimum value of each feature (column) in $X$;\n",
    " - $X_{max}$ is the maximum value of each feature (column) in $X$;\n",
    "\n",
    "This means that the data point with the maximum value in the original data will be scaled to 1. The data point with the minimum value in the original data will be scaled to 0. And all other data points will be scaled proportionally between 0 and 1 based on their relative position within the original data range.\n",
    "\n",
    "While the scaled stock price doesn't show the actual price values, it still represents the stock price movement in a normalized way, allowing us to focus on the trends and patterns and compare them with other scaled features. When we plot the scaled stock price alongside the sentiment scores, we're essentially comparing the trends and patterns of both features over time. We can observe whether positive sentiment tends to coincide with upward price movements, negative sentiment with downward movements, or if there are any other interesting relationships."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q5aVi2P35RgJ"
   },
   "source": [
    "# **4. Scenario: Topic Modeling of Financial News**\n",
    "\n",
    "We will now use the News API data to discover underlying topics in financial news articles related to Microsoft and analyze sentiment associated with each topic.\n",
    "\n",
    "**Topic modeling** is an unsupervised machine learning technique used to discover hidden thematic structures or topics within a collection of documents (also known as a **corpus**). It aims to automatically identify groups of words that frequently co-occur in documents, representing underlying themes or subjects. Topic modeling algorithms typically work by assuming that each document is a mixture of a small number of topics, and each topic is characterized by a distribution of words. The goal is to learn these topic distributions and the document-topic assignments.\n",
    "\n",
    "Key terminology:\n",
    "\n",
    " - **Document:** A single unit of text, such as an article, blog post, or tweet.\n",
    " - **Corpus:** A collection of documents.\n",
    " - **Topic:** A hidden thematic structure or theme within the corpus, represented by a distribution of words.\n",
    " - **Document-Topic Distribution:** The probability or weight of each topic within each document.\n",
    " - **Topic-Word Distribution:** The probability or weight of each word within each topic.\n",
    "\n",
    "Non-negative matrix factorization (NMF) is a powerful technique for discovering hidden topics in text data. By applying it to financial news, we can uncover the main themes being discussed about Microsoft. Combining topic modeling with sentiment analysis provides a deeper understanding of the sentiment associated with different topics. This can help identify areas of positive or negative perception toward the company. The insights gained from this analysis can inform investment decisions, risk management, and public relations strategies.\n",
    "\n",
    "**Implementation steps:**\n",
    "\n",
    " - **Data Preparation:** We will use the existing `df` DataFrame containing News API data selecting the 'Description' column as the text data for analysis. We will preprocess the text data (remove stop words, punctuation, stemming/lemmatization).\n",
    "\n",
    " - **Document-Term Matrix (TDM) Creation:** Then we will create a document-term matrix using TF-IDF (Term Frequency-Inverse Document Frequency). This matrix represents the frequency or importance of each word in each document.\n",
    "\n",
    " - **NMF Application:** We apply NMF to the document-term matrix to decompose it into two matrices: $W$ matrix representing the importance of each topic in each document and $H$ matrix representing the importance of each word in each topic.\n",
    "\n",
    " - **Topic Extraction and Interpretation:** Access the topic-word matrix ($H$) in order to examine the top words for each topic. Identify the words with the highest weights in each row of $H$ to understand the theme of each topic. Based on the top words, we give meaningful names to the topics to make them interpretable.\n",
    "\n",
    " - **Document-Topic Assignment:** Obtain the document-topic matrix ($W$) in order to assign dominant topics to documents. For each document, we find the topic with the highest weight in the corresponding row of $W$. This assigns the most prominent topic to each document.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ExngcTkD8duC"
   },
   "source": [
    "## **4.1 Data Preparation**\n",
    "\n",
    "We start with data preparation. We preprocess the 'Description' column using the `preprocess()` function introduced earlier and save each processed description in a new column named 'Processed_Description':"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing 'Description' column in df\n",
    "df.loc[:, 'Processed_Description'] = df['Description'].apply(lambda x: preprocess(x))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f0O7FOQt9KPi"
   },
   "source": [
    "Then, we are ready to apply the Term Frequency-Inverse Document Frequency (TF-IDF) technique.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hBVwnlINFkm6"
   },
   "source": [
    "## **4.2 Feature Extraction - Document-Term Matrix (TDM)**\n",
    "\n",
    "In the following code snippet, we fit the `TfidfVectorizer` vectorizer to our data in the 'Processed_Description' column. We apply the `TfidfVectorizer` vectorizer with the `max_features=250` parameter that limits the vocabulary size to the 250 most frequent words. This helps reduce the dimensionality of the document-term matrix (DTM) and can improve performance. We also use a `stop_words='english'` parameter that removes common English words (like \"the,\" \"a,\" \"is\") from the vocabulary, as they often don't carry much meaningful information. This will convert the text data in 'Processed_Description' into a document-term matrix (DTM) represented by the variable `dtm`:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF-IDF Vectorization\n",
    "vectorizer = TfidfVectorizer(max_features=250, stop_words='english')\n",
    "dtm = vectorizer.fit_transform(df['Processed_Description'])\n",
    "dtm.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GvNsu8Mj__X0"
   },
   "source": [
    "The `dtm` variable now holds a sparse matrix where:\n",
    "- Each row represents a document (a news article in our case).\n",
    "- Each column represents a word (or feature) from the vocabulary.\n",
    "- The values in the matrix represent the TF-IDF score of each word in each document.  \n",
    "- The `dtm` is typically stored as a sparse matrix to save memory because it often contains many zero values.\n",
    "\n",
    "The typical range for `max_features` is usually between 1000 and 5000, but it can vary widely depending on the dataset and task depending on such factors as dataset size, task complexity, computation resources, experimentation, etc. For a dataset such as ours with only about 90 rows, a `max_features` value between 100 and 500 would likely be a good starting point for keeping balance between dimensionality and information.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PzrYyvpsFxhP"
   },
   "source": [
    "## **4.3 NMF Application**\n",
    "\n",
    "After creating the `dtm`, we can now apply NMF to this matrix to discover underlying topics in the financial news articles. We are going to do this with `n_components=5` parameter that specifies the number of topics (or components) we want to extract from the data. In this case, we're aiming for five topics.\n",
    "`random_state=42` sets a random seed for reproducibility. Using the same seed ensures that we get the same results each time we run the code:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NMF\n",
    "nmf_model = NMF(n_components=5, random_state=42) # 5 topics\n",
    "W = nmf_model.fit_transform(dtm)\n",
    "H = nmf_model.components_\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YHKV5vjzDgXn"
   },
   "source": [
    "NMF is used to discover underlying topics in text data. It decomposes the document-term matrix into two lower-dimensional matrices that represent the relationships between words and topics and between topics and documents. Now we have obtained:\n",
    "\n",
    " - $W$ (Document-Topic Matrix) represents the distribution of topics within each document. Each row in $W$ corresponds to a document in corpus, and each column corresponds to a topic discovered by NMF. The values in $W$ indicate the weight or probability of each topic being present in each document.\n",
    " - $H$ (Topic-Word Matrix) represents the distribution of words within each topic. Each row in $H$ corresponds to a topic, and each column corresponds to a word in vocabulary. The values in $H$ indicate the weight or importance of each word within each topic.\n",
    "\n",
    "The `n_components=5` parameter determines the number of topics (or components) we want to extract from data. In our case, we're aiming to discover five distinct topics within the financial news articles related to Microsoft. A smaller number of topics (like five) often leads to more interpretable results. It's easier to understand and assign meaningful labels to five topics compared to, say, 20 or 50 topics. When starting with topic modeling, it's often a good practice to begin with a smaller number of topics and then gradually increase it if needed. This makes it possible to get a general understanding of the main themes in the data before diving into more granular analysis. A smaller number of topics also reduces the computational cost of NMF, making the process faster, especially for larger datasets.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jc4PzKKNF3Qj"
   },
   "source": [
    "## **4.4 Topic Extraction and Interpretation**\n",
    "\n",
    "Let's now examine the top words in each topic to understand the theme of the topic. The goal is to understand the themes or subjects represented by each of the five topics extracted by NMF.\n",
    "\n",
    "Recall that the $H$ matrix stores the weight of each word in each topic. We want to identify the words with the highest weights for each topic, as these words are most representative of the topic's theme. For each topic, we'll display the top $n$ words with the highest weights. This will give us a glimpse into the subject matter of the topic. Based on the top words, we'll try to assign a meaningful label or interpretation to each topic. This requires domain knowledge and careful consideration of the context of the financial news articles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieves the names of the features (words)\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "# Extract top words\n",
    "n_top_words = 10  # Top 10 words\n",
    "for topic_idx, topic in enumerate(H):\n",
    "    print(f\"\\nTopic {topic_idx + 1}:\")\n",
    "    print(\" \".join([feature_names[i]\n",
    "                    for i in topic.argsort()[:-n_top_words - 1:-1]]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mqOrze4uR1YL"
   },
   "source": [
    "We can also plot top words for each topic for better visualization. In the following code snippet, we plot the top 10 words for each topic as bar subplots:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set figure size\n",
    "fig, axes = plt.subplots(1, nmf_model.n_components, figsize=(16, 6), sharex=True)\n",
    "axes = axes.flatten()  # Convert to 1D array for easier indexing\n",
    "\n",
    "# Plot bar subplots for each topic\n",
    "for topic_idx, topic in enumerate(H):\n",
    "    top_features_ind = topic.argsort()[:-n_top_words - 1:-1]\n",
    "    top_features = [feature_names[i] for i in top_features_ind]\n",
    "    weights = topic[top_features_ind]\n",
    "\n",
    "    ax = axes[topic_idx]\n",
    "    ax.barh(top_features, weights, height=0.5, fill='blue')\n",
    "    ax.set_title(f'Topic {topic_idx + 1}', fontdict={'fontsize': 14})\n",
    "    ax.invert_yaxis()\n",
    "    ax.tick_params(axis='both', which='major', labelsize=12)\n",
    "\n",
    "# Set figure attributes (title)\n",
    "fig.suptitle('Top words in topics in NMF model', fontsize=20, y=0.98)\n",
    "fig.tight_layout(h_pad=2.0)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qW98d4BfOv9m"
   },
   "source": [
    "The code snippet's main purpose is to display the top words for each topic extracted by NMF. This helps in interpreting the topics and understanding the themes or subjects they represent.\n",
    "\n",
    "As of this writing, get the following top words for each topic (you might get different results as News API will have another set of news articles by the time you run this code):\n",
    "\n",
    " - **Topic 1: Microsoft Windows and AI Features**\n",
    "\n",
    "   - Top Words: `copilot, plus, feature, new, pcs, features, search, voice, ai, windows`\n",
    "   - Interpretation: This topic appears to be related to new features and enhancements in Microsoft Windows, particularly those involving AI and voice search capabilities. Copilot, a potential new AI assistant, is also a prominent theme.\n",
    "\n",
    " - **Topic 2: Microsoft's Data Centers and Energy Initiatives**\n",
    "\n",
    "   - Top Words: `data, energy, centers, power, nuclear, ai, tech, run, microsoft, carbon`\n",
    "   - Interpretation: This topic seems to focus on Microsoft's data centers and their energy consumption. It might involve discussions about power sources, including nuclear energy, and initiatives to reduce carbon emissions. The use of AI technology in data centers is also a possible theme.\n",
    "\n",
    " - **Topic 3: Microsoft's AR/VR Efforts (Hololens)**\n",
    "\n",
    "   - Top Words: `microsoft, windows, hololens, headsets, according, vr, update, production, uploadvr, 11`\n",
    "   - Interpretation: This topic likely revolves around Microsoft's augmented and virtual reality efforts, specifically focusing on the Hololens headset. It might involve updates on Hololens production, new features, or partnerships with VR-related platforms like UploadVR.\n",
    "\n",
    " - **Topic 4: AI Competition (Microsoft, OpenAI, Google)**\n",
    "\n",
    "   - Top Words: `openai, google, ai, microsoft, stay, date, financial, way, competition, amid`\n",
    "   - Interpretation: This topic centers on the competition in the field of artificial intelligence, primarily involving Microsoft, OpenAI, and Google. It might discuss financial aspects, strategic partnerships, and the overall landscape of the AI race.\n",
    "\n",
    " - **Topic 5: Microsoft Flight Simulator and Gaming**\n",
    "\n",
    "   - Top Words: `flight, game, simulator, microsoft, 2024, test, based, reboot, 2020, office`\n",
    "   - Interpretation: This topic seems to be related to Microsoft Flight Simulator, a popular flight simulation game. It might involve discussions about updates, new features, testing phases, or potential releases in the future. The inclusion of \"office\" is a bit unusual and might require further context to understand its relevance.\n",
    "\n",
    "\n",
    "Based on these interpretations, here's a suggested set of labels for the topics:\n",
    "\n",
    " 1. Windows AI Enhancements\n",
    " 2. Data Center Sustainability\n",
    " 3. Hololens and AR/VR\n",
    " 4. AI Industry Competition\n",
    " 5. Flight Simulator and Gaming\n",
    "\n",
    "These labels provide a concise and informative representation of the themes captured by each topic. Remember that topic interpretation can be subjective, so they can be adjusted based on your specific understanding of the data and context.\n",
    "\n",
    "By carefully analyzing the top words and considering the broader context, we can gain valuable insights into the main themes being discussed in financial news articles related to Microsoft."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JWTYEEWEV89F"
   },
   "source": [
    "## **4.5 Document-Topic Assignment**\n",
    "\n",
    "Now, we'll use `W` to assign the most prominent topic to each document. We can do this by finding the topic (column) with the highest weight for each document (row) in `W`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assigns the dominant topic for each document\n",
    "df['Dominant_Topic'] = W.argmax(axis=1) + 1  # +1 to start topic numbering from 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Iqw1r7feXwnY"
   },
   "source": [
    "Here, `W.argmax(axis=1)` finds the index of the maximum value (highest weight) in each row of $W$. This index corresponds to the dominant topic for that document. And `+ 1` adds 1 to the topic index to start topic numbering from 1 instead of 0 (which is the default in Python indexing).\n",
    "\n",
    "Bar charts can be used to visualize the distribution of topics within individual documents or across the entire corpus. The following bar plot demonstrates average topic distribution across all documents:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize average topic distribution across all documents\n",
    "avg_topic_distribution = W.mean(axis=0)\n",
    "\n",
    "# Define topic labels\n",
    "topic_labels = [\"Windows AI Enhancements\", \"Data Center Sustainability\",\n",
    "                \"Hololens and AR/VR\", \"AI Industry Competition\",\n",
    "                \"Flight Simulator and Gaming\"]\n",
    "\n",
    "# Create bar plot\n",
    "bars = plt.bar(np.arange(nmf_model.n_components) + 1, avg_topic_distribution, color='skyblue')\n",
    "\n",
    "# Add bar labels inside bars\n",
    "for bar, label in zip(bars, topic_labels):\n",
    "    plt.text(bar.get_x() + bar.get_width() / 2, bar.get_height() / 2, label,\n",
    "             ha='center', va='center', rotation=90, color='black', fontsize=8)\n",
    "\n",
    "plt.xticks(np.arange(nmf_model.n_components) + 1)\n",
    "plt.xlabel(\"Topic\")\n",
    "plt.ylabel(\"Average Weight\")\n",
    "plt.title(\"Average Topic Distribution Across All Documents\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "augVe3yTxTcg"
   },
   "source": [
    "This bar plot illustrates the average prevalence of each topic across all documents in the dataset. The height of each bar represents the average weight or importance of that topic across all the documents analyzed. Higher bars indicate topics that are more prominent or discussed more frequently overall.\n",
    "\n",
    "We can observe that \"AI Industry Competition\" is the most prevalent topic, followed by \"Hololens and AR/VR,\" based on the heights of their respective bars.\n",
    "\n",
    "By examining this visualization, we can gain insights into the overall thematic distribution within the dataset. This information will guide further analysis, potentially focusing on documents related to \"AI Industry Competition\" and \"Hololens and AR/VR\" for a more in-depth understanding of these dominant themes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YMe4u5bOmpV1"
   },
   "source": [
    "## **5. Conclusion**\n",
    "\n",
    "In this lesson, we explored different news data sources. We compared and contrasted various sources like yfinance, Google News RSS, and News API. Ultimately, News API was selected for its flexibility and features.\n",
    "\n",
    "Then, we performed sentiment analysis using FinBERT, a pre-trained NLP model for financial text. Sentiment scores (positive, negative, and neutral) were calculated for news articles about Microsoft. The sentiment scores were plotted over time to observe trends and potential correlations with Microsoft's stock price.\n",
    "\n",
    "Finally, we applied topic modeling, using NMF (non-negative matrix factorization) to discover underlying topics or themes within the news data. This involved preprocessing the text, creating a document-term matrix with TF-IDF, and applying the NMF algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "KvTOJQ-GFrc1"
   },
   "source": [
    "---\n",
    "Copyright 2024 WorldQuant University. This\n",
    "content is licensed solely for personal use. Redistribution or\n",
    "publication of this material is strictly prohibited.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
